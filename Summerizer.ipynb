{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from bert_embedding import BertEmbedding\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemma = WordNetLemmatizer()\n",
    "threshold = 20\n",
    "missing_words = 0\n",
    "bert_embedding = BertEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methods for running the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions dictionary for changing the short form words\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for cleaning the dataset:\n",
    "\n",
    "def cleaning(data, remove_stop = False):\n",
    "    new_text = []\n",
    "    pattern = \"[^A-Z a-z]\"\n",
    "    try:\n",
    "        if data != \"\":\n",
    "            br_free = re.sub(\"<br />\", \" \", data)\n",
    "            br_free = br_free.lower()\n",
    "            html_free_text = BeautifulSoup(br_free, 'html.parser').get_text()\n",
    "            only_words = re.sub(pattern, \" \", html_free_text)\n",
    "            lemmatized = \"\".join([lemma.lemmatize(word) for word in only_words])\n",
    "\n",
    "\n",
    "            text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', lemmatized, flags=re.MULTILINE)\n",
    "            text = re.sub(r'\\<a href', ' ', text)\n",
    "            text = re.sub(r'&amp;', ' ', text) \n",
    "            text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "            text = re.sub(r'<br />', ' ', text)\n",
    "            text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "            stop_free = []\n",
    "            if remove_stop:\n",
    "                for word in text.split():\n",
    "                    if word not in stop_words:\n",
    "                        stop_free.append(word)\n",
    "            else:\n",
    "                stop_free = text.split()\n",
    "            for word in stop_free:\n",
    "                if word in contractions:\n",
    "                    new_text.append(contractions[word])\n",
    "                else:\n",
    "                    new_text.append(word)\n",
    "\n",
    "            return \" \".join(new_text)\n",
    "        else:\n",
    "            return \"None\"\n",
    "    except:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary creation:\n",
    "\n",
    "def vocab_dic(text, count_dict):\n",
    "    for word in text.split():\n",
    "        if word in count_dict.keys():\n",
    "            count_dict[word] += 1\n",
    "        else:\n",
    "            count_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding index using glove embedding\n",
    "\n",
    "def embedding_index_creation():\n",
    "    embedding_index = dict()\n",
    "    file = open(\"C://Users//sudhk//Documents//UML//CODES//Machine Learning//Text_Summerizer//Data//glove.42B.300d.txt\", encoding='utf-8')\n",
    "    for line in tqdm(file):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeff = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coeff\n",
    "    file.close()\n",
    "    return embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding matrix for glove:\n",
    "\n",
    "def embedding_matrix_creation(embedding_dim):\n",
    "    embedding_dim = 300\n",
    "    nb_words = len(vocab_to_int)\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    for word,ind in (tqdm(vocab_to_int.items())):\n",
    "        if word in embedding_index:\n",
    "            embedding_matrix[ind] = embedding_index[word]\n",
    "        else:\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            #embedding_index[word] = new_embedding\n",
    "            embedding_matrix[ind] = new_embedding\n",
    "    print(\"\\n\")\n",
    "    print(\"Embedding matrix is created !!!\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embedding matrix for bert:\n",
    "\n",
    "def bert_embedding_matrix_creation():\n",
    "    embedding_dim = 768\n",
    "    nb_words = len(vocab_to_int)\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    for voc,ind in (tqdm(vocab_to_int.items())):\n",
    "        if voc in embedding_index:\n",
    "            word = voc.split()  # Removing the contextual advantage of bert.\n",
    "            embedding_matrix[ind] = bert_embedding(word)[0][1][0]\n",
    "        else:\n",
    "            word = voc.split()\n",
    "            new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "            embedding_matrix[ind] = new_embedding\n",
    "    print(\"\\n\")\n",
    "    print(\"Embedding matrix is created !!!\")\n",
    "    return embedding_matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ints(text, word_count, unk_count, eos = False):\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None, ), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None, ), name='text_length')\n",
    "    \n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_sieze):\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1,1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int[\"<GO>\"]), ending], 1)\n",
    "    \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, input_keep_prob = keep_prob)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, input_keep_prob = keep_prob)\n",
    "            \n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, rnn_inputs, sequence_length, dtype=tf.float32)\n",
    "            \n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embd_input, summary_length, dec_cell, initial_state, output_layer, vocab_size, max_summary_length):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embd_input, sequence_length=summary_length, time_major=False)\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, initial_state, output_layer)\n",
    "    training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return training_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer, max_summary_length, batch_size):\n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings, start_tokens, end_token)\n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, initial_state, output_layer)\n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "        output_layer = Dense(vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size, enc_output, text_length, normalize=False, name='BahdanauAttention')\n",
    "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attn_mech, rnn_size)\n",
    "            \n",
    "        initial_state = dec_cell.zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            training_decoder = training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, vocab_size, max_summary_length)\n",
    "            training_logits, _, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            inference_decoder = inference_decoding_layer(embeddings, vocab_to_int['<GO>'], vocab_to_int['<EOS>'], dec_cell, initial_state, output_layer, max_summary_length, batch_size)\n",
    "            inference_logits,_,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder, output_time_major=False, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "\n",
    "        return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_2_seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_matrix):\n",
    "    embeddings = embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    for batch_i in range(0, len(texts) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        text_batch = texts[start_i:start_i+batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(text_batch))\n",
    "        \n",
    "        pad_summaries_lenghts = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lenghts.append(len(summary))\n",
    "        \n",
    "        pad_texts_lenghts = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lenghts.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lenghts, pad_texts_lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = cleaning(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input_sentence, input_summary, load_model_name):\n",
    "    text = text_to_seq(input_sentence)\n",
    "\n",
    "    checkpoint = load_model_name\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "       # Load saved model\n",
    "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "        loader.restore(sess, checkpoint)\n",
    "\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "        summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        #Multiply by batch_size to match the model's input parameters\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                         summary_length: [np.random.randint(5,8)], \n",
    "                                         text_length: [len(text)]*batch_size,\n",
    "                                         keep_prob: 1.0})[0] \n",
    "\n",
    "    # Remove the padding from the tweet\n",
    "    pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "    print('\\nOriginal Text:', input_sentence)\n",
    "        \n",
    "    print('\\nText')\n",
    "    print('  Word Ids:    {}'.format([i for i in text]))\n",
    "    print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "    print('\\nSummary')\n",
    "    print('  Actual summary: {}'.format(input_summary))\n",
    "    print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "    print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))\n",
    "    return \" \".join([int_to_vocab[i] for i in answer_logits if i != pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Reviews.csv\")\n",
    "dataset = dataset[[\"Summary\", \"Text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10000/10000 [00:42<00:00, 232.97it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = []\n",
    "cleaned_summary = []\n",
    "\n",
    "for ind in tqdm(range(10000)):\n",
    "    cleaned_text.append(cleaning(dataset[\"Text\"][ind], remove_stop=True))\n",
    "    cleaned_summary.append(cleaning(dataset[\"Summary\"][ind], remove_stop=False))\n",
    "\n",
    "cleaned_dataset = pd.DataFrame(columns = [\"Text\", \"Summary\"])\n",
    "cleaned_dataset[\"Summary\"] = cleaned_summary\n",
    "cleaned_dataset[\"Text\"] = cleaned_text\n",
    "cleaned_dataset.to_csv(\"Cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 15186.96it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = []\n",
    "cleaned_summary = []\n",
    "cleaned_dataset = pd.read_csv(\"Cleaned_dataset.csv\")\n",
    "cleaned_dataset = cleaned_dataset[0:1000] # only 1000 is considered for faster computation. \n",
    "\n",
    "for i in tqdm(range(len(cleaned_dataset))):\n",
    "    cleaned_text.append(cleaned_dataset[\"Text\"][i])\n",
    "    cleaned_summary.append(cleaned_dataset[\"Summary\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = cleaned_dataset.dropna() # dropping the empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 11812.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating count dictionary:\n",
    "\n",
    "count_dict = {}\n",
    "for ind in tqdm(range(len(cleaned_dataset))):\n",
    "    vocab_dic(cleaned_dataset[\"Text\"][ind], count_dict)\n",
    "    vocab_dic(cleaned_dataset[\"Summary\"][ind], count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>black market hot sauce wonderful husband loves...</td>\n",
       "      <td>hot flavorful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>man say salsa bomb different kinds almost ever...</td>\n",
       "      <td>great hot sauce and people who run it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>sauce good anything like adding asian food any...</td>\n",
       "      <td>this sauce is the shiznit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>hot like low star reviewer got suckered seeing...</td>\n",
       "      <td>not hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>admit sucker large quantity oz shopping hot sa...</td>\n",
       "      <td>not hot not habanero</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               Text  \\\n",
       "0             0  bought several vitality canned dog food produc...   \n",
       "1             1  product arrived labeled jumbo salted peanuts p...   \n",
       "2             2  confection around centuries light pillowy citr...   \n",
       "3             3  looking secret ingredient robitussin believe f...   \n",
       "4             4  great taffy great price wide assortment yummy ...   \n",
       "..          ...                                                ...   \n",
       "995         995  black market hot sauce wonderful husband loves...   \n",
       "996         996  man say salsa bomb different kinds almost ever...   \n",
       "997         997  sauce good anything like adding asian food any...   \n",
       "998         998  hot like low star reviewer got suckered seeing...   \n",
       "999         999  admit sucker large quantity oz shopping hot sa...   \n",
       "\n",
       "                                   Summary  \n",
       "0                    good quality dog food  \n",
       "1                        not as advertised  \n",
       "2                      delight says it all  \n",
       "3                           cough medicine  \n",
       "4                              great taffy  \n",
       "..                                     ...  \n",
       "995                          hot flavorful  \n",
       "996  great hot sauce and people who run it  \n",
       "997              this sauce is the shiznit  \n",
       "998                                not hot  \n",
       "999                   not hot not habanero  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5903"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [05:41, 5608.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating embedding index:\n",
    "\n",
    "embedding_index = embedding_index_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the missing words:\n",
    "\n",
    "missing_words = 0\n",
    "for word,count in count_dict.items():\n",
    "    if count > 0:\n",
    "        if word not in embedding_index:\n",
    "            missing_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab to int\n",
    "\n",
    "value = 0\n",
    "vocab_to_int = {}\n",
    "\n",
    "for word, count in count_dict.items():\n",
    "    if count >= threshold or word in embedding_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "codes = [\"<UNK>\", \"<EOS>\", \"<PAD>\", \"<GO>\"]\n",
    "value2 = len(vocab_to_int)\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = value2\n",
    "    value2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = {}\n",
    "for i, word in vocab_to_int.items():\n",
    "    int_to_vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y =  train_test_split(cleaned_text, cleaned_summary, random_state = 3, test_size = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 39596\n",
      "Total number of UNKs in headlines: 118\n",
      "Percent of words that are UNK: 0.3%\n"
     ]
    }
   ],
   "source": [
    "# Converting the clean text and summary to ints:\n",
    "\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = to_ints(train_y, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = to_ints(train_x, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length(text):\n",
    "    lengths = []\n",
    "    for sen in text:\n",
    "        lengths.append(len(sen))\n",
    "    return pd.DataFrame(lengths, columns=[\"Counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "           Counts\n",
      "count  990.000000\n",
      "mean     4.157576\n",
      "std      2.791258\n",
      "min      1.000000\n",
      "25%      2.000000\n",
      "50%      4.000000\n",
      "75%      5.000000\n",
      "max     30.000000\n",
      "\n",
      "Texts:\n",
      "           Counts\n",
      "count  990.000000\n",
      "mean    36.838384\n",
      "std     34.812950\n",
      "min      6.000000\n",
      "25%     17.000000\n",
      "50%     26.000000\n",
      "75%     44.000000\n",
      "max    471.000000\n"
     ]
    }
   ],
   "source": [
    "length_summary = create_length(int_summaries)\n",
    "length_texts = create_length(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(length_summary.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(length_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove reviews that include too many unknowns:\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 100\n",
    "unk_summary_limit = 70\n",
    "\n",
    "for length in range(min(length_texts.Counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction using glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 17711/17711 [00:00<00:00, 203234.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Embedding matrix is created !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating glove embedding matrix:\n",
    "\n",
    "glove_embedding_matrix = embedding_matrix_creation(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model_name = \"glove.ckpt\" \n",
    "glove_load_model = \"./glove.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "\n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq_2_seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size,\n",
    "                                                       glove_embedding_matrix)\n",
    "\n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10 Batch   20/253 - Loss:  4.277, Seconds: 9.15\n",
      "Epoch   1/10 Batch   40/253 - Loss:  3.030, Seconds: 7.63\n",
      "Epoch   1/10 Batch   60/253 - Loss:  2.889, Seconds: 9.89\n",
      "Epoch   1/10 Batch   80/253 - Loss:  2.756, Seconds: 8.56\n",
      "Average loss for this update: 3.226\n",
      "New Record!\n",
      "Epoch   1/10 Batch  100/253 - Loss:  2.762, Seconds: 7.59\n",
      "Epoch   1/10 Batch  120/253 - Loss:  2.948, Seconds: 9.12\n",
      "Epoch   1/10 Batch  140/253 - Loss:  2.859, Seconds: 8.91\n",
      "Epoch   1/10 Batch  160/253 - Loss:  2.902, Seconds: 11.74\n",
      "Average loss for this update: 2.872\n",
      "New Record!\n",
      "Epoch   1/10 Batch  180/253 - Loss:  2.747, Seconds: 12.54\n",
      "Epoch   1/10 Batch  200/253 - Loss:  2.743, Seconds: 14.55\n",
      "Epoch   1/10 Batch  220/253 - Loss:  2.993, Seconds: 15.83\n",
      "Epoch   1/10 Batch  240/253 - Loss:  2.984, Seconds: 17.96\n",
      "Average loss for this update: 2.854\n",
      "New Record!\n",
      "Epoch   2/10 Batch   20/253 - Loss:  2.356, Seconds: 9.38\n",
      "Epoch   2/10 Batch   40/253 - Loss:  2.211, Seconds: 7.48\n",
      "Epoch   2/10 Batch   60/253 - Loss:  2.199, Seconds: 9.64\n",
      "Epoch   2/10 Batch   80/253 - Loss:  2.180, Seconds: 8.25\n",
      "Average loss for this update: 2.238\n",
      "New Record!\n",
      "Epoch   2/10 Batch  100/253 - Loss:  2.192, Seconds: 7.39\n",
      "Epoch   2/10 Batch  120/253 - Loss:  2.329, Seconds: 8.85\n",
      "Epoch   2/10 Batch  140/253 - Loss:  2.323, Seconds: 8.66\n",
      "Epoch   2/10 Batch  160/253 - Loss:  2.371, Seconds: 11.42\n",
      "Average loss for this update: 2.309\n",
      "No Improvement.\n",
      "Epoch   2/10 Batch  180/253 - Loss:  2.251, Seconds: 12.85\n",
      "Epoch   2/10 Batch  200/253 - Loss:  2.306, Seconds: 14.28\n",
      "Epoch   2/10 Batch  220/253 - Loss:  2.497, Seconds: 15.95\n",
      "Epoch   2/10 Batch  240/253 - Loss:  2.525, Seconds: 17.92\n",
      "Average loss for this update: 2.393\n",
      "No Improvement.\n",
      "Epoch   3/10 Batch   20/253 - Loss:  2.015, Seconds: 9.37\n",
      "Epoch   3/10 Batch   40/253 - Loss:  1.888, Seconds: 7.28\n",
      "Epoch   3/10 Batch   60/253 - Loss:  1.894, Seconds: 9.83\n",
      "Epoch   3/10 Batch   80/253 - Loss:  1.898, Seconds: 8.43\n",
      "Average loss for this update: 1.925\n",
      "New Record!\n",
      "Epoch   3/10 Batch  100/253 - Loss:  1.904, Seconds: 7.53\n",
      "Epoch   3/10 Batch  120/253 - Loss:  2.014, Seconds: 8.89\n",
      "Epoch   3/10 Batch  140/253 - Loss:  2.021, Seconds: 8.53\n",
      "Epoch   3/10 Batch  160/253 - Loss:  2.061, Seconds: 11.36\n",
      "Average loss for this update: 2.006\n",
      "No Improvement.\n",
      "Epoch   3/10 Batch  180/253 - Loss:  1.961, Seconds: 12.34\n",
      "Epoch   3/10 Batch  200/253 - Loss:  2.044, Seconds: 14.57\n",
      "Epoch   3/10 Batch  220/253 - Loss:  2.182, Seconds: 15.74\n",
      "Epoch   3/10 Batch  240/253 - Loss:  2.232, Seconds: 18.32\n",
      "Average loss for this update: 2.107\n",
      "No Improvement.\n",
      "Epoch   4/10 Batch   20/253 - Loss:  1.802, Seconds: 9.56\n",
      "Epoch   4/10 Batch   40/253 - Loss:  1.655, Seconds: 7.35\n",
      "Epoch   4/10 Batch   60/253 - Loss:  1.624, Seconds: 9.96\n",
      "Epoch   4/10 Batch   80/253 - Loss:  1.692, Seconds: 8.22\n",
      "Average loss for this update: 1.695\n",
      "New Record!\n",
      "Epoch   4/10 Batch  100/253 - Loss:  1.704, Seconds: 7.31\n",
      "Epoch   4/10 Batch  120/253 - Loss:  1.785, Seconds: 8.83\n",
      "Epoch   4/10 Batch  140/253 - Loss:  1.821, Seconds: 8.82\n",
      "Epoch   4/10 Batch  160/253 - Loss:  1.829, Seconds: 11.51\n",
      "Average loss for this update: 1.792\n",
      "No Improvement.\n",
      "Epoch   4/10 Batch  180/253 - Loss:  1.800, Seconds: 13.11\n",
      "Epoch   4/10 Batch  200/253 - Loss:  1.900, Seconds: 14.68\n",
      "Epoch   4/10 Batch  220/253 - Loss:  1.985, Seconds: 15.87\n",
      "Epoch   4/10 Batch  240/253 - Loss:  2.044, Seconds: 19.15\n",
      "Average loss for this update: 1.931\n",
      "No Improvement.\n",
      "Epoch   5/10 Batch   20/253 - Loss:  1.572, Seconds: 9.69\n",
      "Epoch   5/10 Batch   40/253 - Loss:  1.463, Seconds: 7.51\n",
      "Epoch   5/10 Batch   60/253 - Loss:  1.476, Seconds: 9.83\n",
      "Epoch   5/10 Batch   80/253 - Loss:  1.493, Seconds: 8.28\n",
      "Average loss for this update: 1.502\n",
      "New Record!\n",
      "Epoch   5/10 Batch  100/253 - Loss:  1.536, Seconds: 7.53\n",
      "Epoch   5/10 Batch  120/253 - Loss:  1.623, Seconds: 8.99\n",
      "Epoch   5/10 Batch  140/253 - Loss:  1.648, Seconds: 8.74\n",
      "Epoch   5/10 Batch  160/253 - Loss:  1.668, Seconds: 11.60\n",
      "Average loss for this update: 1.626\n",
      "No Improvement.\n",
      "Epoch   5/10 Batch  180/253 - Loss:  1.639, Seconds: 12.85\n",
      "Epoch   5/10 Batch  200/253 - Loss:  1.760, Seconds: 14.65\n",
      "Epoch   5/10 Batch  220/253 - Loss:  1.780, Seconds: 16.15\n",
      "Epoch   5/10 Batch  240/253 - Loss:  1.857, Seconds: 11.02\n",
      "Average loss for this update: 1.761\n",
      "No Improvement.\n",
      "Epoch   6/10 Batch   20/253 - Loss:  1.389, Seconds: 6.64\n",
      "Epoch   6/10 Batch   40/253 - Loss:  1.320, Seconds: 5.10\n",
      "Epoch   6/10 Batch   60/253 - Loss:  1.308, Seconds: 7.00\n",
      "Epoch   6/10 Batch   80/253 - Loss:  1.377, Seconds: 6.02\n",
      "Average loss for this update: 1.35\n",
      "New Record!\n",
      "Epoch   6/10 Batch  100/253 - Loss:  1.388, Seconds: 5.26\n",
      "Epoch   6/10 Batch  120/253 - Loss:  1.464, Seconds: 6.44\n",
      "Epoch   6/10 Batch  140/253 - Loss:  1.471, Seconds: 6.58\n",
      "Epoch   6/10 Batch  160/253 - Loss:  1.485, Seconds: 8.88\n",
      "Average loss for this update: 1.463\n",
      "No Improvement.\n",
      "Epoch   6/10 Batch  180/253 - Loss:  1.525, Seconds: 10.92\n",
      "Epoch   6/10 Batch  200/253 - Loss:  1.608, Seconds: 12.66\n",
      "Epoch   6/10 Batch  220/253 - Loss:  1.654, Seconds: 13.58\n",
      "Epoch   6/10 Batch  240/253 - Loss:  1.729, Seconds: 14.56\n",
      "Average loss for this update: 1.636\n",
      "No Improvement.\n",
      "Epoch   7/10 Batch   20/253 - Loss:  1.236, Seconds: 6.98\n",
      "Epoch   7/10 Batch   40/253 - Loss:  1.191, Seconds: 6.18\n",
      "Epoch   7/10 Batch   60/253 - Loss:  1.182, Seconds: 7.86\n",
      "Epoch   7/10 Batch   80/253 - Loss:  1.239, Seconds: 8.08\n",
      "Average loss for this update: 1.213\n",
      "New Record!\n",
      "Epoch   7/10 Batch  100/253 - Loss:  1.273, Seconds: 6.16\n",
      "Epoch   7/10 Batch  120/253 - Loss:  1.359, Seconds: 6.88\n",
      "Epoch   7/10 Batch  140/253 - Loss:  1.351, Seconds: 7.54\n",
      "Epoch   7/10 Batch  160/253 - Loss:  1.398, Seconds: 10.10\n",
      "Average loss for this update: 1.355\n",
      "No Improvement.\n",
      "Epoch   7/10 Batch  180/253 - Loss:  1.394, Seconds: 9.76\n",
      "Epoch   7/10 Batch  200/253 - Loss:  1.481, Seconds: 11.02\n",
      "Epoch   7/10 Batch  220/253 - Loss:  1.537, Seconds: 13.00\n",
      "Epoch   7/10 Batch  240/253 - Loss:  1.637, Seconds: 13.94\n",
      "Average loss for this update: 1.521\n",
      "No Improvement.\n",
      "Epoch   8/10 Batch   20/253 - Loss:  1.148, Seconds: 7.06\n",
      "Epoch   8/10 Batch   40/253 - Loss:  1.103, Seconds: 6.58\n",
      "Epoch   8/10 Batch   60/253 - Loss:  1.082, Seconds: 7.46\n",
      "Epoch   8/10 Batch   80/253 - Loss:  1.136, Seconds: 6.36\n",
      "Average loss for this update: 1.119\n",
      "New Record!\n",
      "Epoch   8/10 Batch  100/253 - Loss:  1.189, Seconds: 5.84\n",
      "Epoch   8/10 Batch  120/253 - Loss:  1.240, Seconds: 6.90\n",
      "Epoch   8/10 Batch  140/253 - Loss:  1.238, Seconds: 6.94\n",
      "Epoch   8/10 Batch  160/253 - Loss:  1.282, Seconds: 9.82\n",
      "Average loss for this update: 1.247\n",
      "No Improvement.\n",
      "Epoch   8/10 Batch  180/253 - Loss:  1.294, Seconds: 11.34\n",
      "Epoch   8/10 Batch  200/253 - Loss:  1.387, Seconds: 12.66\n",
      "Epoch   8/10 Batch  220/253 - Loss:  1.427, Seconds: 12.80\n",
      "Epoch   8/10 Batch  240/253 - Loss:  1.495, Seconds: 14.36\n",
      "Average loss for this update: 1.405\n",
      "No Improvement.\n",
      "Epoch   9/10 Batch   20/253 - Loss:  1.040, Seconds: 6.88\n",
      "Epoch   9/10 Batch   40/253 - Loss:  0.998, Seconds: 5.70\n",
      "Epoch   9/10 Batch   60/253 - Loss:  0.973, Seconds: 7.78\n",
      "Epoch   9/10 Batch   80/253 - Loss:  1.044, Seconds: 6.76\n",
      "Average loss for this update: 1.014\n",
      "New Record!\n",
      "Epoch   9/10 Batch  100/253 - Loss:  1.080, Seconds: 6.32\n",
      "Epoch   9/10 Batch  120/253 - Loss:  1.126, Seconds: 7.82\n",
      "Epoch   9/10 Batch  140/253 - Loss:  1.176, Seconds: 7.54\n",
      "Epoch   9/10 Batch  160/253 - Loss:  1.156, Seconds: 10.87\n",
      "Average loss for this update: 1.145\n",
      "No Improvement.\n",
      "Epoch   9/10 Batch  180/253 - Loss:  1.208, Seconds: 11.26\n",
      "Epoch   9/10 Batch  200/253 - Loss:  1.314, Seconds: 13.84\n",
      "Epoch   9/10 Batch  220/253 - Loss:  1.326, Seconds: 13.62\n",
      "Epoch   9/10 Batch  240/253 - Loss:  1.401, Seconds: 14.14\n",
      "Average loss for this update: 1.32\n",
      "No Improvement.\n",
      "Epoch  10/10 Batch   20/253 - Loss:  0.936, Seconds: 7.40\n",
      "Epoch  10/10 Batch   40/253 - Loss:  0.939, Seconds: 5.66\n",
      "Epoch  10/10 Batch   60/253 - Loss:  0.926, Seconds: 7.42\n",
      "Epoch  10/10 Batch   80/253 - Loss:  0.988, Seconds: 6.26\n",
      "Average loss for this update: 0.949\n",
      "New Record!\n",
      "Epoch  10/10 Batch  100/253 - Loss:  0.993, Seconds: 5.68\n",
      "Epoch  10/10 Batch  120/253 - Loss:  1.036, Seconds: 7.16\n",
      "Epoch  10/10 Batch  140/253 - Loss:  1.106, Seconds: 6.92\n",
      "Epoch  10/10 Batch  160/253 - Loss:  1.090, Seconds: 8.96\n",
      "Average loss for this update: 1.063\n",
      "No Improvement.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/10 Batch  180/253 - Loss:  1.126, Seconds: 9.86\n",
      "Epoch  10/10 Batch  200/253 - Loss:  1.226, Seconds: 11.66\n",
      "Epoch  10/10 Batch  220/253 - Loss:  1.241, Seconds: 12.20\n",
      "Epoch  10/10 Batch  240/253 - Loss:  1.299, Seconds: 13.98\n",
      "Average loss for this update: 1.234\n",
      "No Improvement.\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "rnn_size = 216\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3\n",
    "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "avg_loss = [] # Record the average loss.\n",
    "\n",
    "checkpoint = glove_model_name\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                average_loss = round(update_loss/update_check,3)\n",
    "                print(\"Average loss for this update:\", average_loss)\n",
    "                summary_update_loss.append(update_loss)\n",
    "                avg_loss.append(average_loss)\n",
    "\n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "\n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "\n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: gentleman reviewed said go store get better well disabled disability van allows us two packages carry see carrying two big cases water home think wonderful water sure could get sale better price drive someone drive carry please sweet deal ordered six cases watched ups guy set inside door nothing complain price right considered go wrong\n",
      "\n",
      "Text\n",
      "  Word Ids:    [1774, 3757, 2282, 258, 391, 390, 17, 143, 8418, 10429, 8416, 705, 206, 132, 316, 2246, 761, 2272, 132, 772, 3717, 503, 205, 1095, 165, 503, 30, 619, 390, 1738, 17, 99, 2048, 427, 2048, 2246, 439, 323, 105, 89, 306, 3717, 4953, 1711, 248, 1010, 2001, 453, 774, 2840, 99, 167, 2672, 258, 931]\n",
      "  Input Words: gentleman reviewed said go store get better well disabled disability van allows us two packages carry see carrying two big cases water home think wonderful water sure could get sale better price drive someone drive carry please sweet deal ordered six cases watched ups guy set inside door nothing complain price right considered go wrong\n",
      "\n",
      "Summary\n",
      "  Actual summary: delicioius water i didn t have to carry in\n",
      "  Word Ids:       [8, 10]\n",
      "  Response Words: good product\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: adore k cups keto diet hard find pre made drinks loaded sugar guys control sugar content still enjoy perfect glass iced tea\n",
      "\n",
      "Text\n",
      "  Word Ids:    [4997, 1927, 1408, 14753, 264, 448, 212, 2164, 90, 1029, 1024, 54, 1041, 1671, 54, 1178, 418, 1218, 392, 824, 2478, 1183]\n",
      "  Input Words: adore k cups keto diet hard find pre made drinks loaded sugar guys control sugar content still enjoy perfect glass iced tea\n",
      "\n",
      "Summary\n",
      "  Actual summary: perfect for dieters on a hot day\n",
      "  Word Ids:       [8, 10]\n",
      "  Response Words: good product\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: love fact subscribeand save change food many benefits dog loves aided suppression tear staining part beagle part king charles makes poo firm\n",
      "\n",
      "Text\n",
      "  Word Ids:    [173, 521, 17707, 736, 1609, 5, 112, 1539, 4, 310, 9700, 12473, 3672, 9670, 1690, 2831, 1690, 1532, 12474, 197, 7178, 3851]\n",
      "  Input Words: love fact <UNK> save change food many benefits dog loves aided suppression tear staining part beagle part king charles makes poo firm\n",
      "\n",
      "Summary\n",
      "  Actual summary: great food\n",
      "  Word Ids:       [8, 4, 5]\n",
      "  Response Words: good dog food\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: excited see gluten free bisquick grandmother mother used fondly remember strawberry shortcake first thing made almost cried good since made pancakes yum yum baked chicken regular web site liveglutenfreely com great recipes thank bisquick\n",
      "\n",
      "Text\n",
      "  Word Ids:    [2725, 761, 625, 469, 5726, 5703, 486, 866, 11983, 415, 303, 11930, 277, 552, 90, 620, 3596, 8, 758, 90, 3494, 431, 431, 1853, 1443, 267, 3572, 4242, 17707, 1529, 97, 1943, 234, 5726]\n",
      "  Input Words: excited see gluten free bisquick grandmother mother used fondly remember strawberry shortcake first thing made almost cried good since made pancakes yum yum baked chicken regular web site <UNK> com great recipes thank bisquick\n",
      "\n",
      "Summary\n",
      "  Actual summary: wow pancake satisfaction at last\n",
      "  Word Ids:       [158, 12, 1365, 5726]\n",
      "  Response Words: just like old bisquick\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: trying several wu yi teas like one particularly subtle citrus taste also natural sweetness ingredients include organic wu li cliff oolong tea mg organic black tea mg organic green tea extract mg proprietary blend mg plus ginseng panax orange peel lemon grass guarana ingredients natual orange citrus flavors goal drink tea less coffee enjoying trying different kind tea every day coming back one\n",
      "\n",
      "Text\n",
      "  Word Ids:    [490, 1, 2549, 2550, 1541, 12, 198, 2490, 2418, 43, 231, 181, 530, 571, 193, 2551, 529, 2549, 2552, 2553, 2554, 1183, 2555, 529, 123, 1183, 2555, 529, 358, 1183, 88, 2555, 2556, 1274, 2555, 556, 2557, 2558, 1282, 2559, 984, 172, 2560, 193, 2561, 1282, 43, 113, 2562, 977, 1183, 501, 483, 2563, 490, 275, 199, 1183, 186, 660, 2275, 204, 198]\n",
      "  Input Words: trying several wu yi teas like one particularly subtle citrus taste also natural sweetness ingredients include organic wu li cliff oolong tea mg organic black tea mg organic green tea extract mg proprietary blend mg plus ginseng panax orange peel lemon grass guarana ingredients natual orange citrus flavors goal drink tea less coffee enjoying trying different kind tea every day coming back one\n",
      "\n",
      "Summary\n",
      "  Actual summary: really nice taste\n",
      "  Word Ids:       [35, 293]\n",
      "  Response Words: not fans\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: love ease green natural tea anywhear time tastes great love stash green tea\n",
      "\n",
      "Text\n",
      "  Word Ids:    [173, 1669, 358, 530, 1183, 17707, 330, 549, 97, 173, 648, 358, 1183]\n",
      "  Input Words: love ease green natural tea <UNK> time tastes great love stash green tea\n",
      "\n",
      "Summary\n",
      "  Actual summary: great product\n",
      "  Word Ids:       [97, 2989, 358, 1183]\n",
      "  Response Words: great acai green tea\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: course know delicious ghirardelli chocolate unfortunately purchased two gift bags online found local store half price lesson learned\n",
      "\n",
      "Text\n",
      "  Word Ids:    [879, 188, 299, 5148, 475, 290, 386, 132, 1227, 413, 947, 7, 924, 391, 621, 99, 1140, 1139]\n",
      "  Input Words: course know delicious ghirardelli chocolate unfortunately purchased two gift bags online found local store half price lesson learned\n",
      "\n",
      "Summary\n",
      "  Actual summary: great chocolate\n",
      "  Word Ids:       [475, 475, 5189]\n",
      "  Response Words: chocolate chocolate liquor\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: thanks lot review starflakes made ice cream yet trying daughter also allergic cows milk soy found works well recipes support heavy cream buttermilk flavor like pancakes mashed potatoes person bad review lot trouble transitioning milks currently mix whole plus quart fresh milk make gallon half weaker concentration suggested daughter also demands heat least lukewarm sec microwave thank starflakes explanation taste better warm also sometimes add pint goat yogurt blend usually half gallon portion bought cans condensed emergency use tried use trip drink though normal blend even blending quart fresh milk go cans week started buying case local store keep demand even frequently buy waiting next case hopefully online supply reliable addition costing less\n",
      "\n",
      "Text\n",
      "  Word Ids:    [510, 726, 1580, 17707, 90, 1211, 1247, 790, 490, 309, 181, 1328, 13019, 630, 4466, 7, 1003, 143, 1943, 1459, 1257, 1247, 3499, 93, 12, 3494, 5738, 3239, 733, 723, 1580, 726, 1655, 9939, 8983, 406, 783, 729, 556, 13094, 298, 630, 355, 2811, 621, 2306, 6931, 647, 309, 181, 13848, 557, 661, 13455, 8830, 534, 234, 17707, 11694, 231, 17, 1213, 181, 649, 631, 3510, 8681, 3829, 1274, 493, 621, 2811, 3073, 0, 1385, 9633, 4851, 233, 276, 233, 202, 977, 553, 1407, 1274, 522, 11229, 13094, 298, 630, 258, 1385, 261, 1014, 163, 46, 924, 391, 362, 3282, 522, 3153, 438, 4202, 769, 46, 3916, 947, 2249, 8192, 85, 9553, 501]\n",
      "  Input Words: thanks lot review <UNK> made ice cream yet trying daughter also allergic cows milk soy found works well recipes support heavy cream buttermilk flavor like pancakes mashed potatoes person bad review lot trouble transitioning milks currently mix whole plus quart fresh milk make gallon half weaker concentration suggested daughter also demands heat least lukewarm sec microwave thank <UNK> explanation taste better warm also sometimes add pint goat yogurt blend usually half gallon portion bought cans condensed emergency use tried use trip drink though normal blend even blending quart fresh milk go cans week started buying case local store keep demand even frequently buy waiting next case hopefully online supply reliable addition costing less\n",
      "\n",
      "Summary\n",
      "  Actual summary: all that and less than our local store\n",
      "  Word Ids:       [77, 639, 8]\n",
      "  Response Words: it s good\n",
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: morinu soft tofu makes best puddings also make vegan mayonaise salad dressings works great\n",
      "\n",
      "Text\n",
      "  Word Ids:    [17707, 139, 2183, 197, 238, 16416, 181, 355, 2177, 16507, 3100, 2034, 1003, 97]\n",
      "  Input Words: <UNK> soft tofu makes best puddings also make vegan mayonaise salad dressings works great\n",
      "\n",
      "Summary\n",
      "  Actual summary: good\n",
      "  Word Ids:       [9198, 17707, 9209]\n",
      "  Response Words: thomy <UNK> senf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./glove.ckpt\n",
      "\n",
      "Original Text: stuff taste like black licorice sweet way tastes kind like thick nasty soy sauce see anyone could take tablespoons week barely managed swallow teaspoon put mouth eat healthy much sugar tastebuds say way stuff daily iron make attempts hide something else though really imagine right addition cap tightly package arrived black goo leaked packaging wrapped securely bubble wrap though got still pleasant clean container could open\n",
      "\n",
      "Text\n",
      "  Word Ids:    [551, 231, 12, 123, 124, 323, 664, 549, 199, 12, 567, 443, 4466, 196, 761, 507, 619, 329, 6823, 261, 1786, 3710, 2329, 4486, 245, 792, 171, 179, 121, 54, 4073, 430, 664, 551, 2704, 904, 355, 11665, 3278, 637, 626, 553, 220, 976, 167, 85, 7882, 3185, 757, 21, 123, 4795, 7141, 1210, 142, 296, 1829, 1830, 553, 84, 418, 1913, 1572, 1837, 619, 1568]\n",
      "  Input Words: stuff taste like black licorice sweet way tastes kind like thick nasty soy sauce see anyone could take tablespoons week barely managed swallow teaspoon put mouth eat healthy much sugar tastebuds say way stuff daily iron make attempts hide something else though really imagine right addition cap tightly package arrived black goo leaked packaging wrapped securely bubble wrap though got still pleasant clean container could open\n",
      "\n",
      "Summary\n",
      "  Actual summary: yuck\n",
      "  Word Ids:       [35, 36, 37]\n",
      "  Response Words: not as advertised\n"
     ]
    }
   ],
   "source": [
    "glove_history = []\n",
    "# To predict all the test data: len(test_x), For easy run only 10 predictions are done.\n",
    "for i in range(10):\n",
    "    glove_history.append(prediction(test_x[i], test_y[i], glove_load_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction using  BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5818/5818 [35:38<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Embedding matrix is created !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_embedding_matrix = bert_embedding_matrix_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert.ckpt\" \n",
    "bert_load_model = \"./bert.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "rnn_size = 216\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "\n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq_2_seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size,\n",
    "                                                        bert_embedding_matrix)\n",
    "\n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 5.243\n",
      "New Record!\n",
      "Average loss for this update: 3.132\n",
      "New Record!\n",
      "Epoch   1/10 Batch   20/25 - Loss:  3.907, Seconds: 10.86\n",
      "Average loss for this update: 3.281\n",
      "No Improvement.\n",
      "Average loss for this update: 2.78\n",
      "New Record!\n",
      "Average loss for this update: 2.753\n",
      "New Record!\n",
      "Epoch   2/10 Batch   20/25 - Loss:  2.808, Seconds: 8.61\n",
      "Average loss for this update: 2.934\n",
      "No Improvement.\n",
      "Average loss for this update: 2.651\n",
      "New Record!\n",
      "Average loss for this update: 2.568\n",
      "New Record!\n",
      "Epoch   3/10 Batch   20/25 - Loss:  2.669, Seconds: 8.79\n",
      "Average loss for this update: 2.833\n",
      "No Improvement.\n",
      "Average loss for this update: 2.517\n",
      "New Record!\n",
      "Average loss for this update: 2.539\n",
      "No Improvement.\n",
      "Epoch   4/10 Batch   20/25 - Loss:  2.546, Seconds: 9.91\n",
      "Average loss for this update: 2.623\n",
      "No Improvement.\n",
      "Average loss for this update: 2.434\n",
      "New Record!\n",
      "Average loss for this update: 2.355\n",
      "New Record!\n",
      "Epoch   5/10 Batch   20/25 - Loss:  2.431, Seconds: 10.05\n",
      "Average loss for this update: 2.545\n",
      "No Improvement.\n",
      "Average loss for this update: 2.283\n",
      "New Record!\n",
      "Average loss for this update: 2.272\n",
      "New Record!\n",
      "Epoch   6/10 Batch   20/25 - Loss:  2.304, Seconds: 10.19\n",
      "Average loss for this update: 2.394\n",
      "No Improvement.\n",
      "Average loss for this update: 2.196\n",
      "New Record!\n",
      "Average loss for this update: 2.175\n",
      "New Record!\n",
      "Epoch   7/10 Batch   20/25 - Loss:  2.199, Seconds: 10.19\n",
      "Average loss for this update: 2.254\n",
      "No Improvement.\n",
      "Average loss for this update: 2.072\n",
      "New Record!\n",
      "Average loss for this update: 2.049\n",
      "New Record!\n",
      "Epoch   8/10 Batch   20/25 - Loss:  2.079, Seconds: 10.47\n",
      "Average loss for this update: 2.144\n",
      "No Improvement.\n",
      "Average loss for this update: 1.956\n",
      "New Record!\n",
      "Average loss for this update: 1.946\n",
      "New Record!\n",
      "Epoch   9/10 Batch   20/25 - Loss:  1.968, Seconds: 10.21\n",
      "Average loss for this update: 2.026\n",
      "No Improvement.\n",
      "Average loss for this update: 1.903\n",
      "New Record!\n",
      "Average loss for this update: 1.869\n",
      "New Record!\n",
      "Epoch  10/10 Batch   20/25 - Loss:  1.890, Seconds: 10.08\n",
      "Average loss for this update: 1.92\n",
      "No Improvement.\n"
     ]
    }
   ],
   "source": [
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "bert_avg_loss = [] # Record the averag loss.\n",
    "\n",
    "checkpoint = bert_model_name\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries, sorted_texts, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                average_loss = round(update_loss/update_check,3)\n",
    "                print(\"Average loss for this update:\", average_loss)\n",
    "                summary_update_loss.append(update_loss)\n",
    "                bert_avg_loss.append(average_loss)\n",
    "\n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "\n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "\n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: pros extremely fragrant full bodied steeped twice nice air tight container organic cons price tao tea blue flower earl grey black tea best earl grey ever tasted would recommend anyone tao tea blue flower earl grey black tea loose leaf ounce tins pack\n",
      "\n",
      "Text\n",
      "  Word Ids:    [2239, 687, 2489, 280, 2462, 4495, 2921, 137, 3474, 3475, 1837, 529, 1554, 99, 4496, 1183, 360, 2914, 2455, 2456, 123, 1183, 238, 2455, 2456, 374, 1050, 134, 60, 507, 4496, 1183, 360, 2914, 2455, 2456, 123, 1183, 2470, 4497, 260, 2170, 414]\n",
      "  Input Words: pros extremely fragrant full bodied steeped twice nice air tight container organic cons price tao tea blue flower earl grey black tea best earl grey ever tasted would recommend anyone tao tea blue flower earl grey black tea loose leaf ounce tins pack\n",
      "\n",
      "Summary\n",
      "  Actual summary: best earl grey ever\n",
      "  Word Ids:       [97, 1846]\n",
      "  Response Words: great cornmeal\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: bold blend great taste flavor comes bursting usually brew drink organic sumatra mandeling bj use blend exclusively get cup rivals complex flavor tassimo brewer fantastic come amazon add subscription service\n",
      "\n",
      "Text\n",
      "  Word Ids:    [4987, 1274, 97, 231, 93, 759, 4384, 493, 901, 977, 529, 4988, 5814, 4989, 233, 1274, 4256, 390, 627, 4990, 2491, 93, 4991, 4992, 1493, 1048, 395, 631, 4617, 237]\n",
      "  Input Words: bold blend great taste flavor comes bursting usually brew drink organic sumatra <UNK> bj use blend exclusively get cup rivals complex flavor tassimo brewer fantastic come amazon add subscription service\n",
      "\n",
      "Summary\n",
      "  Actual summary: good tasting cup o joe\n",
      "  Word Ids:       [191, 1846]\n",
      "  Response Words: unique cornmeal\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: know everyone else got broken received came wrapped plastic bubble wrap single jar broken son loves price great issues\n",
      "\n",
      "Text\n",
      "  Word Ids:    [188, 156, 626, 84, 1816, 441, 295, 142, 2211, 1829, 1830, 732, 1519, 1816, 308, 310, 99, 97, 1390]\n",
      "  Input Words: know everyone else got broken received came wrapped plastic bubble wrap single jar broken son loves price great issues\n",
      "\n",
      "Summary\n",
      "  Actual summary: no issues\n",
      "  Word Ids:       [97, 1846]\n",
      "  Response Words: great cornmeal\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: huge supply still working plenty spare much effective buying one grocery store every time want mint\n",
      "\n",
      "Text\n",
      "  Word Ids:    [482, 2249, 418, 1438, 933, 2250, 121, 961, 163, 198, 622, 391, 186, 330, 221, 2237]\n",
      "  Input Words: huge supply still working plenty spare much effective buying one grocery store every time want mint\n",
      "\n",
      "Summary\n",
      "  Actual summary: these mints are awesome\n",
      "  Word Ids:       [97, 129, 1210, 456, 5590, 248, 1343]\n",
      "  Response Words: great kids packaging for eb guy allergy\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: smiles thank lord found another healthy snack food unsalted tastes great almost gave finding potato chip like thing would ask kettle change texture slightly hard crunchy problem would prefer softer kettle make happen please keep everything else thank\n",
      "\n",
      "Text\n",
      "  Word Ids:    [4296, 234, 4213, 7, 1101, 179, 1189, 5, 29, 549, 97, 620, 1435, 449, 1302, 2097, 12, 552, 134, 1353, 3613, 1609, 588, 891, 448, 3280, 1035, 134, 1689, 1105, 3613, 355, 148, 439, 362, 925, 626, 234]\n",
      "  Input Words: smiles thank lord found another healthy snack food unsalted tastes great almost gave finding potato chip like thing would ask kettle change texture slightly hard crunchy problem would prefer softer kettle make happen please keep everything else thank\n",
      "\n",
      "Summary\n",
      "  Actual summary: smiles\n",
      "  Word Ids:       [3729, 3735]\n",
      "  Response Words: barbeque perfection\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: best caramel chocolate combination ever tasted caramel liquid complements ghiradelli chocolate perfectly try brew attempting lose weight end addict like ghiradelli product far better caramello hershey product every way caramello chocolate lacks refined taste ghiradelli squares caramello caramel harder sublime taste comparing two like comparing warren buffett terms wealth unfortunately caramello bar regard least try ghiradelli\n",
      "\n",
      "Text\n",
      "  Word Ids:    [238, 2885, 475, 192, 374, 1050, 2885, 2947, 5161, 5162, 475, 3229, 605, 901, 5163, 243, 244, 727, 393, 12, 5162, 10, 1086, 17, 5164, 350, 10, 186, 664, 5164, 475, 5165, 2895, 231, 5162, 50, 5164, 2885, 3754, 5166, 231, 5167, 132, 12, 5167, 5168, 5169, 1479, 5170, 290, 5164, 1928, 5171, 661, 605, 5162]\n",
      "  Input Words: best caramel chocolate combination ever tasted caramel liquid complements ghiradelli chocolate perfectly try brew attempting lose weight end addict like ghiradelli product far better caramello hershey product every way caramello chocolate lacks refined taste ghiradelli squares caramello caramel harder sublime taste comparing two like comparing warren buffett terms wealth unfortunately caramello bar regard least try ghiradelli\n",
      "\n",
      "Summary\n",
      "  Actual summary: heavenly action\n",
      "  Word Ids:       [299, 196]\n",
      "  Response Words: delicious sauce\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: like vernor drink time old fashioned drink michigan since labeling known lovely green gold colors gnome character look actually called ginger soda instead ale different typical ginger ale taste vernors strong vanilla presence smoother moist sweeter ginger ale closer cream soda ginger ale good ginger ale dry higher carbonation spicier bite vernors grams sugar per versus grams per schweppes ginger ale neither caffeine recipe changed bit years mostly choice sweeteners use high fructose corn syrup cost cutting mega soda companies use rather cane sugar used use side corn syrup syrup drowns flavors many microbrews using cane sugar although costs get better taste vernor still aged oak barrels years doubt accounting smoothness great soda anytime soda also known like ginger drinks use home remedy upset stomach vernor regional soda many areas country find sells may selling glass bottles soon least page says day writing comment search amazon asin b jk seg see mean vernors quite history two books amazon soda first vernor ginger ale images america michigan isbn second book vernor story gnomes isbn book company marketing green gold packaging famous gnome vernor fans\n",
      "\n",
      "Text\n",
      "  Word Ids:    [12, 4540, 977, 330, 1365, 4551, 977, 4552, 758, 4553, 710, 4011, 358, 944, 4253, 4554, 2463, 828, 26, 2789, 1753, 92, 477, 4541, 275, 1906, 1753, 4541, 231, 4555, 894, 1262, 4264, 1974, 1768, 1252, 1753, 4541, 3369, 1247, 92, 1753, 4541, 8, 1753, 4541, 363, 251, 1001, 4556, 1485, 4555, 2423, 54, 673, 4557, 2423, 673, 4558, 1753, 4541, 1703, 1973, 1338, 1315, 120, 272, 168, 738, 3043, 233, 542, 543, 544, 545, 1405, 3611, 4559, 92, 2607, 233, 981, 541, 54, 866, 233, 1146, 544, 545, 545, 4560, 113, 112, 3791, 1040, 541, 54, 1552, 3423, 390, 17, 231, 4540, 418, 4545, 4546, 4561, 272, 871, 4562, 4563, 97, 92, 2662, 92, 181, 710, 12, 1753, 1029, 233, 205, 4564, 2385, 1337, 4540, 4565, 92, 112, 2670, 400, 212, 4566, 591, 72, 824, 1817, 1314, 661, 1557, 76, 660, 3388, 4567, 4568, 395, 4569, 2764, 4570, 4571, 761, 219, 4555, 749, 4572, 132, 4573, 395, 92, 277, 4540, 1753, 4541, 4574, 715, 4552, 4575, 595, 370, 4540, 64, 4576, 4575, 370, 351, 4577, 358, 944, 1210, 3604, 4554, 4540, 293]\n",
      "  Input Words: like vernor drink time old fashioned drink michigan since labeling known lovely green gold colors gnome character look actually called ginger soda instead ale different typical ginger ale taste vernors strong vanilla presence smoother moist sweeter ginger ale closer cream soda ginger ale good ginger ale dry higher carbonation spicier bite vernors grams sugar per versus grams per schweppes ginger ale neither caffeine recipe changed bit years mostly choice sweeteners use high fructose corn syrup cost cutting mega soda companies use rather cane sugar used use side corn syrup syrup drowns flavors many microbrews using cane sugar although costs get better taste vernor still aged oak barrels years doubt accounting smoothness great soda anytime soda also known like ginger drinks use home remedy upset stomach vernor regional soda many areas country find sells may selling glass bottles soon least page says day writing comment search amazon asin b jk seg see mean vernors quite history two books amazon soda first vernor ginger ale images america michigan isbn second book vernor story gnomes isbn book company marketing green gold packaging famous gnome vernor fans\n",
      "\n",
      "Summary\n",
      "  Actual summary: made in michigan since\n",
      "  Word Ids:       [97, 1846]\n",
      "  Response Words: great cornmeal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: recently acquired bottle immediately impressed good flavor without excessive saltiness supermarket brands used marinades dipping sauces addition straight use cases added little something extra quite surprise find american made soy sauce tastes good\n",
      "\n",
      "Text\n",
      "  Word Ids:    [1505, 3592, 201, 1652, 472, 8, 93, 559, 4969, 4947, 746, 160, 866, 4970, 3409, 2033, 85, 2160, 233, 3717, 764, 487, 637, 2094, 749, 1386, 212, 3350, 90, 4466, 196, 549, 8]\n",
      "  Input Words: recently acquired bottle immediately impressed good flavor without excessive saltiness supermarket brands used marinades dipping sauces addition straight use cases added little something extra quite surprise find american made soy sauce tastes good\n",
      "\n",
      "Summary\n",
      "  Actual summary: better than anything in the supermarket\n",
      "  Word Ids:       [97, 1846]\n",
      "  Response Words: great cornmeal\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: thing marmite like hate love live without much english thing language even adopted trade name metaphor victoria beckham marmite meaning like etc amazon done world favor making weird wonderful stuff available affordable price last time looked g jar british stores l county x costs england amazon supplier quick efficient enough marmite last end year would decide send one two jars son japan good job amazon\n",
      "\n",
      "Text\n",
      "  Word Ids:    [552, 3580, 12, 1244, 173, 1760, 559, 121, 1428, 552, 3581, 522, 3582, 3583, 2929, 3584, 3137, 3585, 3580, 2106, 12, 118, 395, 702, 240, 3586, 1250, 2938, 165, 551, 704, 2485, 99, 796, 330, 797, 1533, 1519, 3587, 773, 1839, 3588, 685, 3423, 3589, 395, 3590, 103, 2911, 920, 3580, 796, 727, 1164, 134, 547, 913, 198, 132, 1271, 308, 3591, 8, 1901, 395]\n",
      "  Input Words: thing marmite like hate love live without much english thing language even adopted trade name metaphor victoria beckham marmite meaning like etc amazon done world favor making weird wonderful stuff available affordable price last time looked g jar british stores l county x costs england amazon supplier quick efficient enough marmite last end year would decide send one two jars son japan good job amazon\n",
      "\n",
      "Summary\n",
      "  Actual summary: an acquired taste\n",
      "  Word Ids:       [97, 129, 93, 1210, 1210, 1210, 1210]\n",
      "  Response Words: great kids flavor packaging packaging packaging packaging\n",
      "INFO:tensorflow:Restoring parameters from ./bert.ckpt\n",
      "\n",
      "Original Text: okay eat potato chips anyone spicy would find hard eat many makes indulgence rescues people overindulgence\n",
      "\n",
      "Text\n",
      "  Word Ids:    [1181, 171, 1302, 2919, 507, 867, 134, 212, 448, 171, 112, 197, 3287, 3877, 834, 3878]\n",
      "  Input Words: okay eat potato chips anyone spicy would find hard eat many makes indulgence rescues people overindulgence\n",
      "\n",
      "Summary\n",
      "  Actual summary: an indulgence with a bite\n",
      "  Word Ids:       [97, 1846]\n",
      "  Response Words: great cornmeal\n"
     ]
    }
   ],
   "source": [
    "bert_history = []\n",
    "# To predict all the test data: len(test_x), For easy run only 10 predictions are done.\n",
    "for i in range(10):\n",
    "    bert_history.append(prediction(test_x[i], test_y[i], bert_load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the output:\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    print(\"Actual Summary: {}\".format(test_y[i]))\n",
    "    print(\"Glove Summary:  {}\".format(glove_history[i]))\n",
    "    print(\"Bert Summary:   {}\".format(bert_history[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average loss:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(bert_avg_loss, \"r\")\n",
    "plt.plot(avg_loss, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
